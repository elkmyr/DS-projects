{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jigsaw Unintended Bias in Toxicity Classification\n",
    "### Detect toxicity across a diverse range of conversations\n",
    "\n",
    "\n",
    "\n",
    "***Nicolas Hubert***\n",
    "\n",
    "This dataset comes from the following Kaggle challenge : https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data.\n",
    "The key objective of this challenge is to design machine learning models so that they can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion.\n",
    "\n",
    "We will mostly use this dataset to experiment with NB-SVM model, presented by Sida Wang and Christopher D. Manning in the following paper : https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import re, string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0   \n",
       "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9  00040093b2687caa  alignment on this subject and which are contra...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  \n",
       "5             0        0       0       0              0  \n",
       "6             1        1       0       1              0  \n",
       "7             0        0       0       0              0  \n",
       "8             0        0       0       0              0  \n",
       "9             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nMore\\nI can\\'t make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It\\'s listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['comment_text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This challenge comes from Kaggle. We do have a test set. However, the test set is unlabelled and in order for us to measure the accuracy of our model, we are going to split the train set into two separate new ones : a (new) train set, and a validation set on which our model will be assessed. This is because we are mostly interested in what algorithm is likely to best discriminate new data instances. The test set will only be relevant when it comes to real predictions and Kaggle ranking (which is not the main point here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 29303,  34581,  11521, ..., 105104,  37742,  10492])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm = np.random.permutation(df_train.shape[0])\n",
    "perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29303</th>\n",
       "      <td>4dbae79ce2f0adf0</td>\n",
       "      <td>]]. It's not displaying the new uploaded image...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34581</th>\n",
       "      <td>5c5d0d958058864a</td>\n",
       "      <td>Portuguese name \\n\\nOne fact should be noted. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11521</th>\n",
       "      <td>1e762e02b2368c56</td>\n",
       "      <td>\"\\n\\nBrian Redban Reichle Deletion\\nYou are te...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90473</th>\n",
       "      <td>f21b4a6e7f36ebfb</td>\n",
       "      <td>\"\\nYour hairsplitting distinction between \"\"ru...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>012158a8030c1bd0</td>\n",
       "      <td>Men's ranking Figure: While nice to look at, m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                       comment_text  \\\n",
       "29303  4dbae79ce2f0adf0  ]]. It's not displaying the new uploaded image...   \n",
       "34581  5c5d0d958058864a  Portuguese name \\n\\nOne fact should be noted. ...   \n",
       "11521  1e762e02b2368c56  \"\\n\\nBrian Redban Reichle Deletion\\nYou are te...   \n",
       "90473  f21b4a6e7f36ebfb  \"\\nYour hairsplitting distinction between \"\"ru...   \n",
       "444    012158a8030c1bd0  Men's ranking Figure: While nice to look at, m...   \n",
       "\n",
       "       toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "29303      0             0        0       0       0              0  \n",
       "34581      0             0        0       0       0              0  \n",
       "11521      1             0        0       0       0              0  \n",
       "90473      0             0        0       0       0              0  \n",
       "444        0             0        0       0       0              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.iloc[perm]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "delim = int(0.7*df_train.shape[0])\n",
    "train = df_train.iloc[0:delim,:]\n",
    "valid = df_train.iloc[delim:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111699, 47872)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape[0], valid.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>111699.000000</td>\n",
       "      <td>111699.000000</td>\n",
       "      <td>111699.000000</td>\n",
       "      <td>111699.00000</td>\n",
       "      <td>111699.000000</td>\n",
       "      <td>111699.000000</td>\n",
       "      <td>111699.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.097091</td>\n",
       "      <td>0.010304</td>\n",
       "      <td>0.053573</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.050045</td>\n",
       "      <td>0.008809</td>\n",
       "      <td>0.896928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.296083</td>\n",
       "      <td>0.100987</td>\n",
       "      <td>0.225173</td>\n",
       "      <td>0.05541</td>\n",
       "      <td>0.218039</td>\n",
       "      <td>0.093444</td>\n",
       "      <td>0.304054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene        threat  \\\n",
       "count  111699.000000  111699.000000  111699.000000  111699.00000   \n",
       "mean        0.097091       0.010304       0.053573       0.00308   \n",
       "std         0.296083       0.100987       0.225173       0.05541   \n",
       "min         0.000000       0.000000       0.000000       0.00000   \n",
       "25%         0.000000       0.000000       0.000000       0.00000   \n",
       "50%         0.000000       0.000000       0.000000       0.00000   \n",
       "75%         0.000000       0.000000       0.000000       0.00000   \n",
       "max         1.000000       1.000000       1.000000       1.00000   \n",
       "\n",
       "              insult  identity_hate           none  \n",
       "count  111699.000000  111699.000000  111699.000000  \n",
       "mean        0.050045       0.008809       0.896928  \n",
       "std         0.218039       0.093444       0.304054  \n",
       "min         0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       1.000000  \n",
       "50%         0.000000       0.000000       1.000000  \n",
       "75%         0.000000       0.000000       1.000000  \n",
       "max         1.000000       1.000000       1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "# label 'none' for those that do not have any label at all.\n",
    "# Please note we are taking the max on all labels \n",
    "# In other words, as soon as a comment has any given label, it cannot be 'none'\n",
    "train['none'] = 1 - train[label_cols].max(axis=1)\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394.4141039758637, 591.3371226932044, 5000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = train.comment_text.str.len()\n",
    "lens.mean(), lens.std(), lens.max() # std is very high !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, `re.compile(f'...')` compiles the expression `([{string.punctuation}...])` into pattern objects and the `f'...'` prefix causes the formatted string literal (f-string) `{string.punctuation}` to be evaluated at run-time. `string.punctuation` is a string that already contains common punctuation marks.\n",
    "\n",
    "Then `re_tok.sub(r' \\1 ', s)` finds all the matching punctuations and adds a prefix and suffix of white-spaces. Lastly, `split()` tokenizes strings into an array of individual words and punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "\n",
    "def tokenize(s): \n",
    "    return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `vec` object below :\n",
    "\n",
    "* strip_accents : Remove accents and perform other character normalization during the preprocessing step.\n",
    "\n",
    "* ngram_range : The lower and upper boundary of the range of n-values for different n-grams to be extracted. For example an ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
    "\n",
    "* max_df : When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "\n",
    "* min_df : When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "\n",
    "* use_idfbool : Enable inverse-document-frequency reweighting.\n",
    "\n",
    "* smooth_idfbool : Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.\n",
    "\n",
    "* sublinear_tfbool : Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = train.shape[0]\n",
    "\n",
    "vec = TfidfVectorizer(strip_accents='unicode', ngram_range=(1,2), tokenizer=tokenize,\n",
    "               max_df=0.8, min_df=2, use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )\n",
    "x_train = vec.fit_transform(train.comment_text)\n",
    "x_valid = vec.transform(valid.comment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<111699x524117 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 12576331 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB-SVM Model\n",
    "\n",
    "For more information, please have a look at : https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf\n",
    "\n",
    "We define the log-count ratio $r$ for each word $f$:\n",
    "\n",
    "$r = \\log \\frac{\\text{ratio of feature $f$ in positive documents}}{\\text{ratio of feature $f$ in negative documents}}$\n",
    "\n",
    "where ratio of feature $f$ in positive documents is the number of times a positive document has a feature divided by the number of positive documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for y_i = 1, computes the ratio of feature f in positive documents. Same applies for negative document, if y_i = 0.\n",
    "def pr(y_i, y, x):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fit a new model for one of every 6 features, one after the other\n",
    "def get_model(y, x):\n",
    "    y = y.values\n",
    "    r = np.log(pr(1,y,x) / pr(0,y,x))\n",
    "    m = LogisticRegression(C=4, dual=True, penalty='l2', solver='liblinear')\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y), r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "fit severe_toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit obscene\n",
      "fit threat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit insult\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit identity_hate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "preds = np.zeros((len(valid), len(label_cols)))\n",
    "\n",
    "for i, j in enumerate(label_cols):\n",
    "    print('fit', j)\n",
    "    m,r = get_model(train[j], x_train)\n",
    "    preds[:,i] = m.predict_proba(x_valid.multiply(r))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.around(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = valid.iloc[:,2:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9830137310606061"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds == val).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
